{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from urllib2 import urlopen\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"./../.data/FFChallenge_v2/background.dta\"\n",
    "\n",
    "with open(df_path, \"r\") as f:\n",
    "    df_features = pd.read_stata(f)\n",
    "\n",
    "df_train = df_features.set_index('challengeID')\n",
    "print df_train.shape\n",
    "\n",
    "# Convert to Numeric, If Possible\n",
    "df_train_na = df_train.replace('NA', np.NaN)\n",
    "df_train_na.cf4fint = pd.to_datetime(df_train_na.cf4fint)\n",
    "df_train_na_cols = df_train_na.columns[df_train_na.dtypes == 'object']\n",
    "df_train_na[df_train_na_cols] = df_train_na[df_train_na_cols].apply(lambda x: pd.to_numeric(x, errors = 'ignore'))\n",
    "print df_train_na.shape\n",
    "\n",
    "# Throw out what is still an object -- revisit later if needed.\n",
    "df_train_no_obj = df_train_na[df_train_na.columns[df_train_na.dtypes != 'object']]\n",
    "final_cols = df_train_no_obj.columns[~ df_train_no_obj.isnull().all()]\n",
    "df_final = pd.DataFrame(df_train_no_obj[final_cols])\n",
    "print df_final.shape\n",
    "\n",
    "# Find number of unique values in each column. If unique == 1, then remove from final data frame.\n",
    "n = df_final.apply(lambda x: len(x.unique()))\n",
    "df_final = pd.DataFrame(df_final[df_final.columns[n>1]])\n",
    "print df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, remove these because they're a pain in the ass.\n",
    "garbage = [\n",
    "    'm1f1c', \n",
    "    'cm1povca', \n",
    "    'cf1povca', \n",
    "    'm2b6', \n",
    "    'f4a10h2', \n",
    "    'f4a10h3',\n",
    "    'p5i30a', \n",
    "    'p5h23', \n",
    "    'cm5saliva',\n",
    "    'ck5saliva',\n",
    "    'cf4fint',\n",
    "]\n",
    "mask = [x for x in df_final.columns if x not in garbage]\n",
    "df_final = df_final[mask]\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_categories(x):\n",
    "    if isinstance(x,basestring):\n",
    "        x\n",
    "        return int(x.split(' ')[0])\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step... make sure categorical data is uniformly spaced...\n",
    "df_clean = df_final.applymap(clean_categories)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Brian Features\n",
    "\n",
    "1. Categories that substitute f/m/pcg for equivalent questions (fill values in the following order).\n",
    "\n",
    "        a. Fill in missing values using w/in survey questions.\n",
    "        Solves the problem of different pathways to answering questions; e.g., sometimes same questions are asked depending on different circumstances they can be labeled differently.\n",
    "        \n",
    "        b. Fill in missing values using m-f questions (validated).\n",
    "        Solves the problem of missing values for same year. Seems better than going multi-year away.\n",
    "        \n",
    "        c. Fill in missing values using cross-year questions (validated).\n",
    "        Solves problem of survey drop-outs if no info available from mother on current condition.\n",
    "        \n",
    "        d. Freq. distribution across what is left column-wise.\n",
    "    \n",
    "    **Outcome: This will condense the parent categories into one representative category regardless of the PCG of the child.**\n",
    "    \n",
    "    \n",
    "2. Averages / sums for particular questions.\n",
    "\n",
    "        a. Eviction-Related Questions\n",
    "        b. Job-training questions.\n",
    "        c. materialHardship questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingValuesReport(df):\n",
    "\n",
    "    def getSurveyName(x):\n",
    "        r = re.compile('([a-z]+[0-9])')\n",
    "        m = r.match(x)\n",
    "        if m:\n",
    "            return m.groups()[0]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    nulls = (df < 0).sum()\n",
    "    missing = nulls.sum()\n",
    "    total = df.size\n",
    "    print 'Num. Missing Values: {} of {}, {:.2%}\\n'.format(missing, total, float(missing) / total)\n",
    "    \n",
    "    print 'Top Missing Surveys:'\n",
    "    print nulls.groupby(by=getSurveyName).sum().sort_values(ascending=False).head(10)\n",
    "    \n",
    "missingValuesReport(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. W/in survey treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcDataCleaning import InSurveyLookup\n",
    "reload(InSurveyLookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inSurveyLookup = InSurveyLookup.Lookup(master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_step1 = df_clean.apply(lambda x: inSurveyLookup.fillColumn(df_clean,x.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingValuesReport(df_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_step1.to_csv('../output/imputing/df_step1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Cross M-F Survey Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ffcDataCleaning import MFLookup\n",
    "reload(MFLookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## dict with question on where child spends time:\n",
    "withMF = {\n",
    "    'm1': 'm1a11',\n",
    "    'f1': 'm1a11',\n",
    "    'm2': 'm2a3',\n",
    "    'f2': 'f2a3',\n",
    "    'm3': 'm3a2',\n",
    "    'f3': 'f3a2',\n",
    "    'm4': 'm4a2',\n",
    "    'f4': 'f4a2',\n",
    "    'm5': 'm5a2',\n",
    "    'f5': 'f5a2',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = np.array(df_final.columns)\n",
    "mfLookup = MFLookup.Lookup(master_list)\n",
    "mfLookup.pairMatches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_step2 = df_step1.apply(lambda x: mfLookup.fillColumn(df_step1,x.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_step2.to_csv('../output/imputing/.data/df_step2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_step2 = pd.read_csv('../output/imputing/.data/df_step2.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step2.head()\n",
    "missingValuesReport(df_step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Cross Year Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcDataCleaning import CrossYearLookup\n",
    "reload(CrossYearLookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cyLookup = CrossYearLookup.Lookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyLookup.pairMatches_father"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyLookup.iterCount = 0\n",
    "df_step3 = df_step2.apply(lambda x: cyLookup.fillColumn(df_step2,x.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyLookup.findSimilarCYSurvey('f3b3e_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('(-+|.*)\\s+([0-9]+\\.[0-9]+|-)\\s\\s(.*)')\n",
    "r = re.compile('(-+|.*\\s+)([0-9]+\\.[0-9]+\\s\\s|-\\s\\s|$)(.*)')\n",
    "s0 = '-----------------------'\n",
    "s1 = '  f1e1c2    -       People who currently live in your HH - 2nd gender?'\n",
    "s2 = '* f2k9      29.0    Are you currently looking for a regular job?'\n",
    "s3 = '  m1a16     -       Who does the baby looks like?'\n",
    "m = r.match(s0)\n",
    "print m.groups()\n",
    "m = r.match(s1)\n",
    "print m.groups()\n",
    "m = r.match(s2)\n",
    "print m.groups()\n",
    "m = r.match(s3)\n",
    "print m.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_step3.to_csv('../output/imputing/.data/df_step3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_step3.head(5)\n",
    "missingValuesReport(df_step3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfLookup.iterCount = 0\n",
    "df_step4 = df_step3.apply(lambda x: mfLookup.fillColumn(df_step3,x.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_step4.to_csv('../output/imputing/.data/df_step4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print missingValuesReport(df_step4)\n",
    "df_step4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions from Null Report:\n",
    "\n",
    "The procedure removed just about all null values (except for only a few) in the m/f surveys. Will set the threshold for inclusion in the final data set to isnull < 50; and impute the rest (most frequent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nullReport = df_step4.isnull().sum().sort_values(ascending=False)\n",
    "nullReport.to_csv('../output/nullReport.csv')\n",
    "\n",
    "naReport = (df_step4 < 0).sum().sort_values(ascending=False)\n",
    "naReport.to_csv('../output/naReport.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Null Columns (keep nulls down to 50)..\n",
    "null_threshold = 50\n",
    "cols = df_step4.columns[(df_step4.isnull().sum() < neg_threshold)].tolist()\n",
    "df_step5 = pd.DataFrame(df_step4[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Negative Columns (keep negatives down to 50)..\n",
    "neg_threshold = 500\n",
    "cols = df_step5.columns[((df_step5 < 0).sum() < neg_threshold)].tolist()\n",
    "df_step6 = pd.DataFrame(df_step5[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_step5.shape\n",
    "print df_step6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = imputation.Imputer(strategy=\"most_frequent\")\n",
    "col = df_step6.columns\n",
    "idx = df_step6.index\n",
    "imputed = pd.DataFrame(im.fit_transform(df_step6.values), index = idx, columns = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputed.to_csv('../output/imputing/.data/imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Material Hardship and Eviction (11 Questions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "free_food = {\n",
    "    '5': ['m5f23a', 'f5f23a', 'n5g1a'],\n",
    "    '4': ['m4i23a', 'f4i23a'],\n",
    "    '3': ['m3i23a', 'f3i23a'],\n",
    "    '2': ['m2h19a', 'f2h17a'],\n",
    "}\n",
    "\n",
    "no_food = {\n",
    "    '5': ['m5f23b', 'f5f23b', 'n5g1b'],\n",
    "    '4': ['m4i23c', 'f4i23c'],\n",
    "    '3': ['m3i23a', 'f3i23a'],\n",
    "    '2': ['m2h19c', 'f2h17c'],\n",
    "}\n",
    "\n",
    "full_rent = {\n",
    "    '5': ['m5f23c',\n",
    "    'f5f23c',\n",
    "    'n5g1c'],\n",
    "    \n",
    "    '4': ['m4i23d',\n",
    "    'f4i23d'],\n",
    "\n",
    "    '3': ['m3i23b',\n",
    "    'f3i23b'],\n",
    "\n",
    "    '2': ['m2h19d',\n",
    "    'f2h17d'],\n",
    "}\n",
    "\n",
    "evicted = {\n",
    "    '5': ['m5f23d',\n",
    "    'f5f23d',\n",
    "    'n5g1d'],\n",
    "    \n",
    "    '4': ['m4i23e',\n",
    "    'f4i23e'],\n",
    "\n",
    "    '3': ['m3i23c',\n",
    "    'f3i23c'],\n",
    "    \n",
    "    '2': ['m2h19e',\n",
    "    'f2h17e'],\n",
    "}\n",
    "\n",
    "gas_bill = {\n",
    "    '5': ['m5f23e',\n",
    "    'f5f23e',\n",
    "    'n5g1e'],\n",
    "    \n",
    "    '4': ['m4i23f',\n",
    "    'f4i23f'],\n",
    "    \n",
    "    '3': ['m3i23d',\n",
    "    'f3i23d'],\n",
    "    \n",
    "    '2': ['m2h19f',\n",
    "    'f2h17f'],\n",
    "}\n",
    "\n",
    "turn_off_oil = {\n",
    "    '5': ['m5f23f',\n",
    "    'f5f23f',\n",
    "    'n5g1f'],\n",
    "    '4': ['m4i23g',\n",
    "    'f4i23g'],\n",
    "    '3': ['m3i6f',\n",
    "    'f3i6f'],\n",
    "    '2': ['m2h19g',\n",
    "    'f2h17g'],\n",
    "}\n",
    "\n",
    "borrow_money = {\n",
    "    '5': ['m5f23g',\n",
    "    'f5f23g',\n",
    "    'n5g1g'],\n",
    "    \n",
    "    '4': ['m4i23h',\n",
    "    'f4i23h'],\n",
    "    \n",
    "    '3':['m3i23e',\n",
    "    'f3i23e'],\n",
    "    \n",
    "    '4': ['m2h19i',\n",
    "    'f2h17i'],\n",
    "}\n",
    "\n",
    "move_in_financial = {\n",
    "    '5': ['m5f23h',\n",
    "    'f5f23h',\n",
    "    'n5g1h'],\n",
    "    \n",
    "    '4': ['m4i23i',\n",
    "    'f4i23i'],\n",
    "\n",
    "    '3': ['m3i23f',\n",
    "    'f3i23f'],\n",
    "    \n",
    "    '2': ['m2h19j',\n",
    "    'f2h17j'],\n",
    "}\n",
    "\n",
    "shelter = {\n",
    "    '5': ['m5f23i',\n",
    "    'f5f23i',\n",
    "    'n5g1i'],\n",
    "    \n",
    "    '4': ['m4i23j',\n",
    "    'f4i23j'],\n",
    "    \n",
    "    '3': ['m3i23g',\n",
    "    'f3i23g'],\n",
    "    \n",
    "    '2': ['m2h19k',\n",
    "    'f2h17k'],\n",
    "}\n",
    "\n",
    "no_doctor = {\n",
    "    '5': ['m5f23j',\n",
    "    'f5f23j',\n",
    "    'n5g1j'],\n",
    "    \n",
    "    '4': ['m4i23k',\n",
    "    'f4i23k'],\n",
    "    \n",
    "    '3': ['m3i23h',\n",
    "    'f3i23h'],\n",
    "    \n",
    "    '2': ['m2h19l',\n",
    "    'f2h17l'],\n",
    "}\n",
    "\n",
    "phone_canceled = {\n",
    "    '5': ['m5f23k',\n",
    "    'f5f23k'],\n",
    "\n",
    "    '4': ['m4i23n',\n",
    "    'f4i23n'],\n",
    "    \n",
    "    '3': ['m3i6a',\n",
    "    'f3i6a'],\n",
    "    \n",
    "    '2': ['m2h19h',\n",
    "    'f2h17h'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtlHardship = {\n",
    "    'free_food': free_food,\n",
    "    'no_food': no_food,\n",
    "    'full_rent': full_rent,\n",
    "    'evicted': evicted,\n",
    "    'gas_bill': gas_bill,\n",
    "    'turn_off_oil': turn_off_oil,\n",
    "    'borrow_money': borrow_money,\n",
    "    'move_in_financial': move_in_financial,\n",
    "    'shelter': shelter,\n",
    "    'no_doctor': no_doctor,\n",
    "    'phone_canceled': phone_canceled,\n",
    "}\n",
    "\n",
    "mtlHardship_list = [j for v in mtlHardship.itervalues() for i in v.itervalues() for j in i]\n",
    "marriage_list = {'{}_{}'.format(k1,k2):j for k1, v in mtlHardship.iteritems() for k2, j in v.iteritems()}\n",
    "print marriage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mtlHardship = pd.DataFrame(df_clean[mtlHardship_list])\n",
    "df_mtlHardship[df_mtlHardship == 2] = 0.\n",
    "df_mtlHardship[df_mtlHardship > 0] = 1.\n",
    "\n",
    "df_mtlHardship_agg = pd.DataFrame()\n",
    "for k,v in marriage_list.iteritems():\n",
    "    df_mtlHardship_agg[k] = df_mtlHardship[v].apply(lambda x: x[x >=0].mean() ,axis=1)\n",
    "\n",
    "df_mtlHardship_agg = df_mtlHardship_agg.apply(fixNan, axis=1)\n",
    "df_mtlHardship_agg[df_mtlHardship_agg.isnull()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_n = df_mtlHardship_agg.columns.map(lambda x: int(x.split('_')[-1]))\n",
    "def filt_year_n(x,n):\n",
    "    n = int(n)\n",
    "    return x.index[col_n == n].tolist()\n",
    "\n",
    "df_mtlHardship_agg_total = pd.DataFrame()\n",
    "for n in range(2,6):\n",
    "    print n\n",
    "    df_mtlHardship_agg_total['total_{}'.format(n)] = df_mtlHardship_agg.apply(lambda x: x[filt_year_n(x,n)].sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mtlHardship_agg_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mtlHardship_agg.to_csv('../output/imputing/.data/materialHardship_features.csv')\n",
    "df_mtlHardship_agg_total.to_csv('../output/imputing/.data/materialHardshipTotal_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_mtlHardship.shape\n",
    "print df_mtlHardship_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for K,V in mtlHardship:\n",
    "    {(k, df_clean[v].apply()) for k,v in V}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grit (average of 4 questions)\n",
    "\n",
    "1. I keep at my schoolwork until I'm done with it.\n",
    "2. I make a plan to get something done, I stick with it.\n",
    "3. I finish whatever I begin.\n",
    "4. I am a hard worker.\n",
    "\n",
    "Note: Not much to do about missing values here... just see if you can apply as is and guess on the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grit = [\n",
    "    'k5g1a',\n",
    "    'k5g1b',\n",
    "    'k5g1c',\n",
    "    'k5g1d',\n",
    "    'k5g1e',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Num missing: {}'.format(df_clean[grit].apply(lambda x: (x > 0).all(), axis = 1).sum())\n",
    "print 'Num total: {}'.format(len(df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_grit = df_clean[grit]\n",
    "df_grit = df_grit.apply(fixNan,axis=1)\n",
    "mean = df_grit[(df_grit>=0).all(axis=1)].mean()\n",
    "\n",
    "df_grit_final = pd.DataFrame(df_grit,copy=True)\n",
    "for col in grit:\n",
    "    df_grit_final[col].loc[df_grit_final[col]<0] = mean[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_grit_final.to_csv('../output/imputing/.data/df_grit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layoff\n",
    "\n",
    "1. Since {last time}, have you ever been laid off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = {\n",
    "    '5': ['m5f8a3',\n",
    "    'f5f8a3'],\n",
    "    '4': ['m4i8a3',\n",
    "    'f4i8a3'],\n",
    "    '3': ['m3i8a3',\n",
    "    'f3i8a3'],\n",
    "    '2': ['m2h9a3',\n",
    "    'f2h8a3'],\n",
    "}\n",
    "unemployment_list = list(chain(*[v for k,v in unemployment.items()]))\n",
    "print unemployment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[unemployment_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fixNan(x):\n",
    "    x[x.isnull()] = x[~x.isnull()].mean()\n",
    "    return x\n",
    "\n",
    "df_layoff = pd.DataFrame(df_clean[unemployment_list])\n",
    "df_layoff[df_layoff == 2] = 0.\n",
    "df_layoff[df_layoff > 0] = 1.\n",
    "\n",
    "df_layoff_agg = pd.DataFrame()\n",
    "for k,v in unemployment.iteritems():\n",
    "    df_layoff_agg['layoff_{}'.format(k)] = df_layoff[v].apply(lambda x: x[x >=0].mean() ,axis=1)\n",
    "\n",
    "df_layoff_agg = df_layoff_agg.apply(fixNan, axis=1)\n",
    "df_layoff_agg[df_layoff_agg.isnull()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_layoff_agg.isnull().sum()\n",
    "df_layoff_agg.to_csv('../output/imputing/.data/layoff_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job Training\n",
    "\n",
    "1. Ever taken job skills/training classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_skills = {\n",
    "    '5': ['m5i1',\n",
    "    'm5i3b',\n",
    "    'f5i1',\n",
    "    'f5i3b'],\n",
    "    \n",
    "    '4': ['m4k1',\n",
    "    'm4k3b',\n",
    "    'f4k1',\n",
    "    'f4k3b'],\n",
    "    \n",
    "    '3': ['m3k1',\n",
    "    'm3k3b',\n",
    "    'f3k1',\n",
    "    'f3k3b'],\n",
    "    \n",
    "    '2': ['m2k1',\n",
    "    'm2k3a13',\n",
    "    'f2k2',\n",
    "    'f2k5a13'],\n",
    "}\n",
    "\n",
    "jobSkill_list = list(chain(*[v for k,v in job_skills.items()]))\n",
    "print jobSkill_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[jobSkill_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fixNan(x):\n",
    "    x[x.isnull()] = x[~x.isnull()].mean()\n",
    "    return x\n",
    "\n",
    "df_jobSkill = pd.DataFrame(df_clean[unemployment_list])\n",
    "df_jobSkill[df_jobSkill == 2] = 0.\n",
    "df_jobSkill[df_jobSkill > 0] = 1.\n",
    "\n",
    "df_jobSkill_agg = pd.DataFrame()\n",
    "for k,v in unemployment.iteritems():\n",
    "    df_jobSkill_agg['jobSkill_{}'.format(k)] = df_jobSkill[v].apply(lambda x: x[x >=0].mean() ,axis=1)\n",
    "\n",
    "df_jobSkill_agg = df_jobSkill_agg.apply(fixNan, axis=1)\n",
    "df_jobSkill_agg[df_jobSkill_agg.isnull()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobSkill_agg.isnull().sum()\n",
    "df_jobSkill_agg.to_csv('../output/imputing/.data/jobSkill_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPA\n",
    "\n",
    "1. no precedent\n",
    "2. Probably look at whatever scores are available..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Imputed Set... Naive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputed = pd.read_csv('../output/imputing/.data/df_step4.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputed[imputed < 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputed = imputed[imputed.columns[~imputed.isnull().all()].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = imputation.Imputer(strategy=\"most_frequent\")\n",
    "col = imputed.columns\n",
    "idx = imputed.index\n",
    "imputed2 = pd.DataFrame(im.fit_transform(imputed.values), index = idx, columns = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputed2.to_csv('../output/imputing/.data/imputed_nodrops.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelMaker = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = df_final.columns[df_final.dtypes.apply(lambda x: x.name == 'category')].tolist()\n",
    "mask = list(set(mask) & set(imputed2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputed3 = pd.DataFrame(imputed2,copy=True)\n",
    "imputed3[mask] = imputed3[mask].apply(labelMaker.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputed3.to_csv('../output/imputing/.data/imputed_nodrops-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
